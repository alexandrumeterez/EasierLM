{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8416c738-92e6-4d20-943e-36c1b504a13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81071bd6-63df-48eb-a3af-901621905044",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_bytes_range(source, bytes_start: int, num_bytes: int) -> bytes:\n",
    "    with open(source, \"rb\") as f:\n",
    "        f.seek(bytes_start)\n",
    "        return f.read(num_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce71abfd-1fdd-40d1-a741-87bdab414182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_chunk_from_memmap(path, index: int, dtype=None):\n",
    "    \n",
    "    dtype = np.uint16\n",
    "    chunksize = 1024\n",
    "    item_size = dtype(0).itemsize\n",
    "    bytes_start = index * item_size * chunksize\n",
    "    num_bytes = item_size * chunksize\n",
    "    buffer = get_bytes_range(path, bytes_start, num_bytes)\n",
    "    array = np.frombuffer(buffer, dtype=dtype)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd402a37-e599-45b7-bef4-6a9a36f53709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.uint16(31756)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_read_chunk_from_memmap(path=\"/n/holylfs06/LABS/kempner_shared/Everyone/testbed/text/dolma/tokenized/t5-base/c4/part-00-00001.npy\", index=0).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d51f2d0-87c7-484a-8e78-87d9fd0cc559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Any, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "class MemMapDatasetJAX:\n",
    "    \"\"\"\n",
    "    A local-file dataset that reads contiguous chunks of token IDs from one or more\n",
    "    numpy memory-mapped arrays and returns JAX arrays.\n",
    "\n",
    "    - No torch dependency.\n",
    "    - No S3/R2/Weka; local files only via np.memmap.\n",
    "    - __getitem__ returns dict with jnp arrays.\n",
    "    - If array length is not a multiple of chunk_size, the trailing remainder is ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *paths,\n",
    "        chunk_size: int = 1024,\n",
    "        memmap_dtype: Union[Type[np.uint8], Type[np.uint16], Type[np.uint32], Type[np.uint64]] = np.uint16,\n",
    "        metadata: Optional[Union[List[Dict[str, Any]], Dict[str, Any]]] = None,\n",
    "        include_instance_metadata: bool = True,\n",
    "        generate_attention_mask: bool = False,\n",
    "        generate_doc_lengths: bool = False,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        label_mask_paths = None,\n",
    "        instance_filter_config: Optional[InstanceFilterConfig] = None,\n",
    "    ):\n",
    "        if not paths:\n",
    "            raise ValueError(\"At least one path is required\")\n",
    "\n",
    "        if generate_attention_mask and pad_token_id is None:\n",
    "            raise ValueError(\"'pad_token_id' is required for 'generate_attention_mask'\")\n",
    "\n",
    "        if generate_doc_lengths and eos_token_id is None:\n",
    "            raise ValueError(\"'eos_token_id' is required for 'generate_doc_lengths'\")\n",
    "\n",
    "        if label_mask_paths and len(label_mask_paths) != len(paths):\n",
    "            raise ValueError(\"There must be the same number of 'label_mask_paths' as there are 'paths'\")\n",
    "\n",
    "        if isinstance(metadata, list):\n",
    "            if len(metadata) != len(paths):\n",
    "                raise ValueError(\"'metadata' should have the same length as the number of file paths\")\n",
    "            _metadata = metadata\n",
    "        else:\n",
    "            _metadata = [metadata or {}] * len(paths)\n",
    "\n",
    "        self._memmap_paths = list(paths)\n",
    "        self._metadata: List[Dict[str, Any]] = _metadata\n",
    "        self._label_mask_paths = label_mask_paths\n",
    "        self._chunk_size = chunk_size\n",
    "        self.dtype = memmap_dtype\n",
    "\n",
    "        self._pad_token_id = pad_token_id\n",
    "        self._eos_token_id = eos_token_id\n",
    "        self.instance_filter_config = instance_filter_config\n",
    "\n",
    "        # Open memmaps eagerly so we know sizes and can compute offsets.\n",
    "        self._memmaps: List[np.memmap] = [\n",
    "            np.memmap(path, dtype=self.dtype, mode=\"r\") for path in self._memmap_paths\n",
    "        ]\n",
    "        self._label_memmaps: Optional[List[np.memmap]] = None\n",
    "        if self._label_mask_paths is not None:\n",
    "            self._label_memmaps = [\n",
    "                np.memmap(mp, dtype=np.bool_, mode=\"r\") for mp in self._label_mask_paths\n",
    "            ]\n",
    "            # sanity check equal lengths\n",
    "            for a, b, p, mp in zip(self._memmaps, self._label_memmaps, self._memmap_paths, self._label_mask_paths):\n",
    "                if a.size != b.size:\n",
    "                    raise ValueError(f\"mask file '{mp}' must have same number of elements as '{p}'\")\n",
    "\n",
    "        # Compute chunk counts and global offsets.\n",
    "        self._chunks_per_path: List[int] = [arr.size // self._chunk_size for arr in self._memmaps]\n",
    "        self._offsets: List[Tuple[int, int]] = []\n",
    "        start = 0\n",
    "        for n_chunks in self._chunks_per_path:\n",
    "            end = start + n_chunks\n",
    "            self._offsets.append((start, end))\n",
    "            start = end\n",
    "        self._num_instances = self._offsets[-1][1] if self._offsets else 0\n",
    "\n",
    "    # -------------------------\n",
    "    # Properties\n",
    "    # -------------------------\n",
    "    @property\n",
    "    def chunk_size(self) -> int:\n",
    "        return self._chunk_size\n",
    "\n",
    "    @property\n",
    "    def max_seq_len(self) -> int:\n",
    "        return self._chunk_size\n",
    "\n",
    "    @property\n",
    "    def offsets(self) -> List[Tuple[int, int]]:\n",
    "        return self._offsets\n",
    "\n",
    "    # -------------------------\n",
    "    # Core\n",
    "    # -------------------------\n",
    "    def __len__(self) -> int:\n",
    "        return self._num_instances\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
    "        index = int(index)\n",
    "        if index < 0:\n",
    "            index = len(self) + index\n",
    "        if not (0 <= index < len(self)):\n",
    "            raise IndexError(f\"{index} is out of bounds for dataset of size {len(self)}\")\n",
    "\n",
    "        # find which file this index maps into\n",
    "        memmap_idx = None\n",
    "        local_idx = None\n",
    "        for i, (s, e) in enumerate(self._offsets):\n",
    "            if s <= index < e:\n",
    "                memmap_idx = i\n",
    "                local_idx = index - s\n",
    "                break\n",
    "        assert memmap_idx is not None and local_idx is not None\n",
    "\n",
    "        # slice the memmap\n",
    "        start = local_idx * self._chunk_size\n",
    "        end = start + self._chunk_size\n",
    "\n",
    "        tokens_np = self._memmaps[memmap_idx][start:end]\n",
    "        # Convert to int32 tokens (common choice); change to int64 if needed.\n",
    "        input_ids = jnp.asarray(tokens_np.astype(np.int32), dtype=jnp.int32)\n",
    "\n",
    "        out: Dict[str, Any] = {\"input_ids\": input_ids}\n",
    "\n",
    "        if self._label_memmaps is not None:\n",
    "            label_np = self._label_memmaps[memmap_idx][start:end]\n",
    "            out[\"label_mask\"] = jnp.asarray(label_np, dtype=jnp.bool_)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c970c52f-7438-49e5-93d4-8fb2167a87ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MemMapDatasetJAX(\n",
    "    \"/n/holylfs06/LABS/kempner_shared/Everyone/testbed/text/dolma/tokenized/t5-base/c4/part-00-00001.npy\",\n",
    "    \"/n/holylfs06/LABS/kempner_shared/Everyone/testbed/text/dolma/tokenized/t5-base/c4/part-00-00002.npy\",\n",
    "    \"/n/holylfs06/LABS/kempner_shared/Everyone/testbed/text/dolma/tokenized/t5-base/c4/part-01-00001.npy\",\n",
    "    chunk_size=2048,\n",
    "    memmap_dtype=np.uint16,\n",
    "    generate_attention_mask=True,\n",
    "    pad_token_id=0,\n",
    "    generate_doc_lengths=True,\n",
    "    eos_token_id=1,\n",
    ")\n",
    "\n",
    "sample = ds[0]\n",
    "x = sample[\"input_ids\"]        # jnp.int32 [2048]\n",
    "# m = sample[\"attention_mask\"]   # jnp.int32 [2048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c51eb2dd-e930-4637-97fc-15af61bae716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(31941, dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7a05bd7-177e-40c9-9a28-f7e686e93e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1056342"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dbd75d4b-47e4-4315-9e36-5c1e75f80450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431fb84d-3be3-42e2-ad1c-16e9284f30e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
